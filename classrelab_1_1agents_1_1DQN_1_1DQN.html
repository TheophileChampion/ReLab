<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.6"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ReLab | Documentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸš€</text></svg>">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="doxygen-awesome-darkmode-toggle.js"></script>
<script type="text/javascript">
    DoxygenAwesomeDarkModeToggle.init()
</script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only-darkmode-toggle.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="relab-logo-only.png"/></td>
  <td id="projectalign">
   <div id="projectname">ReLab<span id="projectnumber">&#160;v1.0.0-b</span>
   </div>
   <div id="projectbrief">Reinforcement Learning Benchmarks</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.6 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classrelab_1_1agents_1_1DQN_1_1DQN.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="classrelab_1_1agents_1_1DQN_1_1DQN-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">relab.agents.DQN.DQN Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Implements a Deep Q-Network.  
 <a href="classrelab_1_1agents_1_1DQN_1_1DQN.html#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for relab.agents.DQN.DQN:</div>
<div class="dyncontent">
<div class="center"><img src="classrelab_1_1agents_1_1DQN_1_1DQN__inherit__graph.png" border="0" usemap="#arelab_8agents_8DQN_8DQN_inherit__map" alt="Inheritance graph"/></div>
<map name="arelab_8agents_8DQN_8DQN_inherit__map" id="arelab_8agents_8DQN_8DQN_inherit__map">
<area shape="rect" title="Implements a Deep Q&#45;Network." alt="" coords="357,381,528,407"/>
<area shape="rect" href="classrelab_1_1agents_1_1CDQN_1_1CDQN.html" title="Implements a Categorical Deep Q&#45;Network." alt="" coords="614,5,803,31"/>
<area shape="rect" href="classrelab_1_1agents_1_1DDQN_1_1DDQN.html" title="Implements a Double Deep Q&#45;Network." alt="" coords="613,55,804,80"/>
<area shape="rect" href="classrelab_1_1agents_1_1DuelingDDQN_1_1DuelingDDQN.html" title="Implements a Dueling Double Deep Q&#45;Network." alt="" coords="608,105,809,145"/>
<area shape="rect" href="classrelab_1_1agents_1_1DuelingDQN_1_1DuelingDQN.html" title="Implements a Dueling Deep Q&#45;Network." alt="" coords="613,169,804,209"/>
<area shape="rect" href="classrelab_1_1agents_1_1IQN_1_1IQN.html" title="Implement an Implicit Quantile Network." alt="" coords="629,233,788,259"/>
<area shape="rect" href="classrelab_1_1agents_1_1MDQN_1_1MDQN.html" title="Implements a multistep Deep Q&#45;Network." alt="" coords="611,283,806,308"/>
<area shape="rect" href="classrelab_1_1agents_1_1NoisyCDQN_1_1NoisyCDQN.html" title="Implement a Categorical Deep Q&#45;Network with noisy linear layers (NoisyCDQN)." alt="" coords="577,332,840,357"/>
<area shape="rect" href="classrelab_1_1agents_1_1NoisyDDQN_1_1NoisyDDQN.html" title="Implement a Double DQN with noisy linear layers." alt="" coords="576,381,841,407"/>
<area shape="rect" href="classrelab_1_1agents_1_1NoisyDQN_1_1NoisyDQN.html" title="Implement a DQN with noisy linear layers." alt="" coords="586,431,831,456"/>
<area shape="rect" href="classrelab_1_1agents_1_1PrioritizedDDQN_1_1PrioritizedDDQN.html" title="Implement a Double DQN with prioritized replay buffer." alt="" coords="622,481,795,521"/>
<area shape="rect" href="classrelab_1_1agents_1_1PrioritizedDQN_1_1PrioritizedDQN.html" title="Implement a DQN with prioritized replay buffer." alt="" coords="622,545,795,585"/>
<area shape="rect" href="classrelab_1_1agents_1_1PrioritizedMDQN_1_1PrioritizedMDQN.html" title="Implement a multistep DQN with prioritized replay buffer." alt="" coords="621,609,797,649"/>
<area shape="rect" href="classrelab_1_1agents_1_1QRDQN_1_1QRDQN.html" title="Implement a quantile regression Deep Q&#45;Network." alt="" coords="604,673,813,699"/>
<area shape="rect" href="classrelab_1_1agents_1_1RainbowDQN_1_1RainbowDQN.html" title="Implement a rainbow Deep Q&#45;Network." alt="" coords="610,723,807,763"/>
<area shape="rect" href="classrelab_1_1agents_1_1RainbowIQN_1_1RainbowIQN.html" title="Implement a rainbow implicit quantile network." alt="" coords="613,787,804,827"/>
<area shape="rect" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html" title="The interface that all agents must implement." alt="" coords="103,374,309,414"/>
<area shape="rect" title=" " alt="" coords="5,381,55,407"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for relab.agents.DQN.DQN:</div>
<div class="dyncontent">
<div class="center"><img src="classrelab_1_1agents_1_1DQN_1_1DQN__coll__graph.png" border="0" usemap="#arelab_8agents_8DQN_8DQN_coll__map" alt="Collaboration graph"/></div>
<map name="arelab_8agents_8DQN_8DQN_coll__map" id="arelab_8agents_8DQN_8DQN_coll__map">
<area shape="rect" title="Implements a Deep Q&#45;Network." alt="" coords="23,167,194,192"/>
<area shape="rect" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html" title="The interface that all agents must implement." alt="" coords="5,79,212,119"/>
<area shape="rect" title=" " alt="" coords="84,5,133,31"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a2d91aced72b5b4b01098fba9e85abaf3"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a2d91aced72b5b4b01098fba9e85abaf3">__init__</a> (self, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a2587135d7d9055a60ba69025563769ae">gamma</a>=0.99, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#afbdb81a773ff9c8974fcd3cb0ac6d25d">learning_rate</a>=0.00001, int <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ad9e7a11caa27eca86e0460f84342c3f9">buffer_size</a>=1000000, int <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ace1892c0e7a64acdb36c5cca15f944e0">batch_size</a>=32, int <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ac24fbc141ab886ed3acc8231809dfa3a">learning_starts</a>=200000, Optional[float] <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aaa183522b6ce8b71287e535597a6f04f">kappa</a>=None, int <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aabd2a4bbb70d1ea02543593983318b9d">target_update_interval</a>=40000, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a2d767c31ee310a484c1b532557421776">adam_eps</a>=1.5e-4, int <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a3073c515322440f72bea0dc53858b16f">n_actions</a>=18, Optional[int] <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a5df9017e12bc717732e49b9e9f8ce255">n_atoms</a>=None, Optional[float] <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a8bc4145119334efced67f11d1fa11046">v_min</a>=None, Optional[float] <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a1704a1876e59896f48375bb63e96a2ca">v_max</a>=None, int <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aa1f80ea708eea38b8b46c9c96220abe6">n_steps</a>=1, bool <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#aebe9410ff0c783d702877cba754bdfe4">training</a>=True, <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1ReplayType.html">ReplayType</a> <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a8eaa62eca1d1b02b091befeaa854b4e8">replay_type</a>=<a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1ReplayType.html#a84ebd587efe64396ce0fc381e39406e7">ReplayType.DEFAULT</a>, <a class="el" href="classrelab_1_1agents_1_1DQN_1_1LossType.html">LossType</a> <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aa09f06ba76e978d3571ede38ce61209e">loss_type</a>=<a class="el" href="classrelab_1_1agents_1_1DQN_1_1LossType.html#a7fc4dd09dd09f4cad7e8b2f80525794f">LossType.DQN_SL1</a>, <a class="el" href="classrelab_1_1agents_1_1DQN_1_1NetworkType.html">NetworkType</a> <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ac0794239af64612da5c0ae9a1b68e18b">network_type</a>=<a class="el" href="classrelab_1_1agents_1_1DQN_1_1NetworkType.html#a151e691483dc02332cb23e2d3d9c88a8">NetworkType.DEFAULT</a>, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a892b68fea851e507f510667be9971056">omega</a>=1.0, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ae5523e1136c036b6e1b54f40b7af0ea0">omega_is</a>=1.0, Any <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a019621ab0c19444789434487859875f5">epsilon_schedule</a>=None)</td></tr>
<tr class="memdesc:a2d91aced72b5b4b01098fba9e85abaf3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Create a DQN agent.  <br /></td></tr>
<tr class="separator:a2d91aced72b5b4b01098fba9e85abaf3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa493fd9606ce72d1bad97b67f6f73110"><td class="memItemLeft" align="right" valign="top">Callable&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aa493fd9606ce72d1bad97b67f6f73110">get_loss</a> (self, <a class="el" href="classrelab_1_1agents_1_1DQN_1_1LossType.html">LossType</a> <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aa09f06ba76e978d3571ede38ce61209e">loss_type</a>)</td></tr>
<tr class="memdesc:aa493fd9606ce72d1bad97b67f6f73110"><td class="mdescLeft">&#160;</td><td class="mdescRight">Retrieve the loss requested as parameters.  <br /></td></tr>
<tr class="separator:aa493fd9606ce72d1bad97b67f6f73110"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5bcef6e093e941977ba2716ab7469df5"><td class="memItemLeft" align="right" valign="top">nn.Module&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a5bcef6e093e941977ba2716ab7469df5">get_value_network</a> (self, <a class="el" href="classrelab_1_1agents_1_1DQN_1_1NetworkType.html">NetworkType</a> <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ac0794239af64612da5c0ae9a1b68e18b">network_type</a>)</td></tr>
<tr class="memdesc:a5bcef6e093e941977ba2716ab7469df5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Retrieve the constructor of the value network requested as parameters.  <br /></td></tr>
<tr class="separator:a5bcef6e093e941977ba2716ab7469df5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6c0ef9bd6fccc1147fcc9c3b4adcdb0a"><td class="memItemLeft" align="right" valign="top"><a id="a6c0ef9bd6fccc1147fcc9c3b4adcdb0a" name="a6c0ef9bd6fccc1147fcc9c3b4adcdb0a"></a>
None&#160;</td><td class="memItemRight" valign="bottom"><b>update_target_network</b> (self)</td></tr>
<tr class="memdesc:a6c0ef9bd6fccc1147fcc9c3b4adcdb0a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Synchronize the target with the value network. <br /></td></tr>
<tr class="separator:a6c0ef9bd6fccc1147fcc9c3b4adcdb0a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a84fd3832544a0135720516681e4fff3b"><td class="memItemLeft" align="right" valign="top">ActionType&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a84fd3832544a0135720516681e4fff3b">step</a> (self, ObservationType obs)</td></tr>
<tr class="memdesc:a84fd3832544a0135720516681e4fff3b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Select the next action to perform in the environment.  <br /></td></tr>
<tr class="separator:a84fd3832544a0135720516681e4fff3b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afb3c1f57ed16d94b8cd82bf4ffe24276"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#afb3c1f57ed16d94b8cd82bf4ffe24276">train</a> (self, Env env)</td></tr>
<tr class="memdesc:afb3c1f57ed16d94b8cd82bf4ffe24276"><td class="mdescLeft">&#160;</td><td class="mdescRight">Train the agent in the gym environment passed as parameters.  <br /></td></tr>
<tr class="separator:afb3c1f57ed16d94b8cd82bf4ffe24276"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a95277e85d8860699b182dad545cc53d8"><td class="memItemLeft" align="right" valign="top"><a id="a95277e85d8860699b182dad545cc53d8" name="a95277e85d8860699b182dad545cc53d8"></a>
Optional[Dict[str, Any]]&#160;</td><td class="memItemRight" valign="bottom"><b>learn</b> (self)</td></tr>
<tr class="memdesc:a95277e85d8860699b182dad545cc53d8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Perform one step of gradient descent on the value network. <br /></td></tr>
<tr class="separator:a95277e85d8860699b182dad545cc53d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae48f88cd975789aa822759948024a52d"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ae48f88cd975789aa822759948024a52d">q_learning_loss</a> (self, Tensor obs, Tensor actions, Tensor rewards, Tensor done, Tensor next_obs, Loss loss_fc, bool double_ql=False)</td></tr>
<tr class="memdesc:ae48f88cd975789aa822759948024a52d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the loss of the standard or double Q-learning algorithm.  <br /></td></tr>
<tr class="separator:ae48f88cd975789aa822759948024a52d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1bb2e42715c44dbef27fc53214e46bdf"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a1bb2e42715c44dbef27fc53214e46bdf">categorical_kl_divergence</a> (self, Tensor obs, Tensor actions, Tensor rewards, Tensor done, Tensor next_obs)</td></tr>
<tr class="memdesc:a1bb2e42715c44dbef27fc53214e46bdf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the loss of the categorical algorithm.  <br /></td></tr>
<tr class="separator:a1bb2e42715c44dbef27fc53214e46bdf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6a59bd7deacfd2b50b3bddd6b1593b44"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a6a59bd7deacfd2b50b3bddd6b1593b44">rainbow_loss</a> (self, Tensor obs, Tensor actions, Tensor rewards, Tensor done, Tensor next_obs)</td></tr>
<tr class="memdesc:a6a59bd7deacfd2b50b3bddd6b1593b44"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the loss of the rainbow DQN.  <br /></td></tr>
<tr class="separator:a6a59bd7deacfd2b50b3bddd6b1593b44"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad23433148a895775990e6dc59622fd73"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ad23433148a895775990e6dc59622fd73">quantile_loss</a> (self, Tensor obs, Tensor actions, Tensor rewards, Tensor done, Tensor next_obs, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aaa183522b6ce8b71287e535597a6f04f">kappa</a>=1.0)</td></tr>
<tr class="memdesc:ad23433148a895775990e6dc59622fd73"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the loss of the quantile regression algorithm.  <br /></td></tr>
<tr class="separator:ad23433148a895775990e6dc59622fd73"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1342607ce111ebf505f15cd68d5cdeeb"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a1342607ce111ebf505f15cd68d5cdeeb">implicit_quantile_loss</a> (self, Tensor obs, Tensor actions, Tensor rewards, Tensor done, Tensor next_obs, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aaa183522b6ce8b71287e535597a6f04f">kappa</a>=1.0)</td></tr>
<tr class="memdesc:a1342607ce111ebf505f15cd68d5cdeeb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the loss of the quantile regression algorithm.  <br /></td></tr>
<tr class="separator:a1342607ce111ebf505f15cd68d5cdeeb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6e8746bfe529654d438c361485e32291"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a6e8746bfe529654d438c361485e32291">rainbow_iqn_loss</a> (self, Tensor obs, Tensor actions, Tensor rewards, Tensor done, Tensor next_obs, float <a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#aaa183522b6ce8b71287e535597a6f04f">kappa</a>=1.0)</td></tr>
<tr class="memdesc:a6e8746bfe529654d438c361485e32291"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the loss of the rainbow IQN.  <br /></td></tr>
<tr class="separator:a6e8746bfe529654d438c361485e32291"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5eba6790ccbd1d07171f0c2a5df2f93"><td class="memItemLeft" align="right" valign="top">Tuple[str, Checkpoint]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#ab5eba6790ccbd1d07171f0c2a5df2f93">load</a> (self, Optional[str] checkpoint_name=None, Optional[str] buffer_checkpoint_name=None)</td></tr>
<tr class="memdesc:ab5eba6790ccbd1d07171f0c2a5df2f93"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load an agent from the filesystem.  <br /></td></tr>
<tr class="separator:ab5eba6790ccbd1d07171f0c2a5df2f93"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afaea756d733758ffbba110e0ebea7cdf"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#afaea756d733758ffbba110e0ebea7cdf">as_dict</a> (self)</td></tr>
<tr class="separator:afaea756d733758ffbba110e0ebea7cdf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a90e9b39f00d5c50fcdb6b506c9fd99fd"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html#a90e9b39f00d5c50fcdb6b506c9fd99fd">save</a> (self, str checkpoint_name, Optional[str] buffer_checkpoint_name=None)</td></tr>
<tr class="memdesc:a90e9b39f00d5c50fcdb6b506c9fd99fd"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save the agent on the filesystem.  <br /></td></tr>
<tr class="separator:a90e9b39f00d5c50fcdb6b506c9fd99fd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html">relab.agents.AgentInterface.AgentInterface</a></td></tr>
<tr class="memitem:a37b65628d214627afae8cf7c8f576435 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a37b65628d214627afae8cf7c8f576435">__init__</a> (self, bool <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#aebe9410ff0c783d702877cba754bdfe4">training</a>=True)</td></tr>
<tr class="memdesc:a37b65628d214627afae8cf7c8f576435 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Create an agent.  <br /></td></tr>
<tr class="separator:a37b65628d214627afae8cf7c8f576435 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5dc04ff5e1441ba6216f2f24e8d8c632 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">ActionType&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a5dc04ff5e1441ba6216f2f24e8d8c632">step</a> (self, ObservationType obs)</td></tr>
<tr class="memdesc:a5dc04ff5e1441ba6216f2f24e8d8c632 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Select the next action to perform in the environment.  <br /></td></tr>
<tr class="separator:a5dc04ff5e1441ba6216f2f24e8d8c632 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acb6aee67e6278cd5230ab2808f774577 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#acb6aee67e6278cd5230ab2808f774577">train</a> (self, Env env)</td></tr>
<tr class="memdesc:acb6aee67e6278cd5230ab2808f774577 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Train the agent in the gym environment passed as parameters.  <br /></td></tr>
<tr class="separator:acb6aee67e6278cd5230ab2808f774577 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a162eb28fb9a698f2a58908c4a1d6110e inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">Tuple[str, Checkpoint]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a162eb28fb9a698f2a58908c4a1d6110e">load</a> (self, Optional[str] checkpoint_name=None, Optional[str] buffer_checkpoint_name=None)</td></tr>
<tr class="memdesc:a162eb28fb9a698f2a58908c4a1d6110e inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load an agent from the filesystem.  <br /></td></tr>
<tr class="separator:a162eb28fb9a698f2a58908c4a1d6110e inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4725a741348861a5c9f9395df508f063 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a4725a741348861a5c9f9395df508f063">as_dict</a> (self)</td></tr>
<tr class="separator:a4725a741348861a5c9f9395df508f063 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a19be8462f89a85eb5c6f4dc9c07201f9 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a19be8462f89a85eb5c6f4dc9c07201f9">save</a> (self, str checkpoint_name, Optional[str] buffer_checkpoint_name=None)</td></tr>
<tr class="memdesc:a19be8462f89a85eb5c6f4dc9c07201f9 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Save the agent on the filesystem.  <br /></td></tr>
<tr class="separator:a19be8462f89a85eb5c6f4dc9c07201f9 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5766245d893fd33acda1bb041e6a5173 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a5766245d893fd33acda1bb041e6a5173">demo</a> (self, Env env, str gif_name, int max_frames=10000)</td></tr>
<tr class="memdesc:a5766245d893fd33acda1bb041e6a5173 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Demonstrate the agent policy in the gym environment passed as parameters.  <br /></td></tr>
<tr class="separator:a5766245d893fd33acda1bb041e6a5173 inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4ab3f64e484947cb12ce99df56d9e4a inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#af4ab3f64e484947cb12ce99df56d9e4a">report</a> (self, SupportsFloat reward, bool done, Dict[str, Any] model_losses=None)</td></tr>
<tr class="memdesc:af4ab3f64e484947cb12ce99df56d9e4a inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Keep track of the last episodic rewards, episode length, and time elapse since last training iteration.  <br /></td></tr>
<tr class="separator:af4ab3f64e484947cb12ce99df56d9e4a inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a92338de60d467d97e4c1939844b8533b inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a92338de60d467d97e4c1939844b8533b">log_performance_in_tensorboard</a> (self)</td></tr>
<tr class="memdesc:a92338de60d467d97e4c1939844b8533b inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Log the agent performance in tensorboard, if the internal queue.  <br /></td></tr>
<tr class="separator:a92338de60d467d97e4c1939844b8533b inherit pub_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a2587135d7d9055a60ba69025563769ae"><td class="memItemLeft" align="right" valign="top"><a id="a2587135d7d9055a60ba69025563769ae" name="a2587135d7d9055a60ba69025563769ae"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>gamma</b></td></tr>
<tr class="memdesc:a2587135d7d9055a60ba69025563769ae"><td class="mdescLeft">&#160;</td><td class="mdescRight">Discount factor for future rewards (between 0 and 1). <br /></td></tr>
<tr class="separator:a2587135d7d9055a60ba69025563769ae"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afbdb81a773ff9c8974fcd3cb0ac6d25d"><td class="memItemLeft" align="right" valign="top"><a id="afbdb81a773ff9c8974fcd3cb0ac6d25d" name="afbdb81a773ff9c8974fcd3cb0ac6d25d"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>learning_rate</b></td></tr>
<tr class="memdesc:afbdb81a773ff9c8974fcd3cb0ac6d25d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Learning rate for the optimizer. <br /></td></tr>
<tr class="separator:afbdb81a773ff9c8974fcd3cb0ac6d25d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad9e7a11caa27eca86e0460f84342c3f9"><td class="memItemLeft" align="right" valign="top"><a id="ad9e7a11caa27eca86e0460f84342c3f9" name="ad9e7a11caa27eca86e0460f84342c3f9"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>buffer_size</b></td></tr>
<tr class="memdesc:ad9e7a11caa27eca86e0460f84342c3f9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Maximum number of transitions stored in the replay buffer. <br /></td></tr>
<tr class="separator:ad9e7a11caa27eca86e0460f84342c3f9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace1892c0e7a64acdb36c5cca15f944e0"><td class="memItemLeft" align="right" valign="top"><a id="ace1892c0e7a64acdb36c5cca15f944e0" name="ace1892c0e7a64acdb36c5cca15f944e0"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>batch_size</b></td></tr>
<tr class="memdesc:ace1892c0e7a64acdb36c5cca15f944e0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of transitions sampled per learning update. <br /></td></tr>
<tr class="separator:ace1892c0e7a64acdb36c5cca15f944e0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aabd2a4bbb70d1ea02543593983318b9d"><td class="memItemLeft" align="right" valign="top"><a id="aabd2a4bbb70d1ea02543593983318b9d" name="aabd2a4bbb70d1ea02543593983318b9d"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>target_update_interval</b></td></tr>
<tr class="memdesc:aabd2a4bbb70d1ea02543593983318b9d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of training steps between target network updates. <br /></td></tr>
<tr class="separator:aabd2a4bbb70d1ea02543593983318b9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac24fbc141ab886ed3acc8231809dfa3a"><td class="memItemLeft" align="right" valign="top"><a id="ac24fbc141ab886ed3acc8231809dfa3a" name="ac24fbc141ab886ed3acc8231809dfa3a"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>learning_starts</b></td></tr>
<tr class="memdesc:ac24fbc141ab886ed3acc8231809dfa3a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Step count at which learning begins. <br /></td></tr>
<tr class="separator:ac24fbc141ab886ed3acc8231809dfa3a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaa183522b6ce8b71287e535597a6f04f"><td class="memItemLeft" align="right" valign="top"><a id="aaa183522b6ce8b71287e535597a6f04f" name="aaa183522b6ce8b71287e535597a6f04f"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>kappa</b></td></tr>
<tr class="memdesc:aaa183522b6ce8b71287e535597a6f04f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Parameter for the quantile Huber loss (used in QR-DQN). <br /></td></tr>
<tr class="separator:aaa183522b6ce8b71287e535597a6f04f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2d767c31ee310a484c1b532557421776"><td class="memItemLeft" align="right" valign="top"><a id="a2d767c31ee310a484c1b532557421776" name="a2d767c31ee310a484c1b532557421776"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>adam_eps</b></td></tr>
<tr class="memdesc:a2d767c31ee310a484c1b532557421776"><td class="mdescLeft">&#160;</td><td class="mdescRight">Epsilon parameter for the Adam optimizer. <br /></td></tr>
<tr class="separator:a2d767c31ee310a484c1b532557421776"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5df9017e12bc717732e49b9e9f8ce255"><td class="memItemLeft" align="right" valign="top"><a id="a5df9017e12bc717732e49b9e9f8ce255" name="a5df9017e12bc717732e49b9e9f8ce255"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>n_atoms</b></td></tr>
<tr class="memdesc:a5df9017e12bc717732e49b9e9f8ce255"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of atoms used to approximate the return distribution. <br /></td></tr>
<tr class="separator:a5df9017e12bc717732e49b9e9f8ce255"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8bc4145119334efced67f11d1fa11046"><td class="memItemLeft" align="right" valign="top"><a id="a8bc4145119334efced67f11d1fa11046" name="a8bc4145119334efced67f11d1fa11046"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>v_min</b></td></tr>
<tr class="memdesc:a8bc4145119334efced67f11d1fa11046"><td class="mdescLeft">&#160;</td><td class="mdescRight">Minimum value for the return distribution support. <br /></td></tr>
<tr class="separator:a8bc4145119334efced67f11d1fa11046"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1704a1876e59896f48375bb63e96a2ca"><td class="memItemLeft" align="right" valign="top"><a id="a1704a1876e59896f48375bb63e96a2ca" name="a1704a1876e59896f48375bb63e96a2ca"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>v_max</b></td></tr>
<tr class="memdesc:a1704a1876e59896f48375bb63e96a2ca"><td class="mdescLeft">&#160;</td><td class="mdescRight">Maximum value for the return distribution support. <br /></td></tr>
<tr class="separator:a1704a1876e59896f48375bb63e96a2ca"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3073c515322440f72bea0dc53858b16f"><td class="memItemLeft" align="right" valign="top"><a id="a3073c515322440f72bea0dc53858b16f" name="a3073c515322440f72bea0dc53858b16f"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>n_actions</b></td></tr>
<tr class="memdesc:a3073c515322440f72bea0dc53858b16f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of possible actions in the environment. <br /></td></tr>
<tr class="separator:a3073c515322440f72bea0dc53858b16f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa1f80ea708eea38b8b46c9c96220abe6"><td class="memItemLeft" align="right" valign="top"><a id="aa1f80ea708eea38b8b46c9c96220abe6" name="aa1f80ea708eea38b8b46c9c96220abe6"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>n_steps</b></td></tr>
<tr class="memdesc:aa1f80ea708eea38b8b46c9c96220abe6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Number of steps for multi-step learning. <br /></td></tr>
<tr class="separator:aa1f80ea708eea38b8b46c9c96220abe6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a892b68fea851e507f510667be9971056"><td class="memItemLeft" align="right" valign="top"><a id="a892b68fea851e507f510667be9971056" name="a892b68fea851e507f510667be9971056"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>omega</b></td></tr>
<tr class="memdesc:a892b68fea851e507f510667be9971056"><td class="mdescLeft">&#160;</td><td class="mdescRight">Exponent for prioritization in the replay buffer. <br /></td></tr>
<tr class="separator:a892b68fea851e507f510667be9971056"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae5523e1136c036b6e1b54f40b7af0ea0"><td class="memItemLeft" align="right" valign="top"><a id="ae5523e1136c036b6e1b54f40b7af0ea0" name="ae5523e1136c036b6e1b54f40b7af0ea0"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>omega_is</b></td></tr>
<tr class="memdesc:ae5523e1136c036b6e1b54f40b7af0ea0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Exponent for importance sampling correction. <br /></td></tr>
<tr class="separator:ae5523e1136c036b6e1b54f40b7af0ea0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8eaa62eca1d1b02b091befeaa854b4e8"><td class="memItemLeft" align="right" valign="top"><a id="a8eaa62eca1d1b02b091befeaa854b4e8" name="a8eaa62eca1d1b02b091befeaa854b4e8"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>replay_type</b></td></tr>
<tr class="memdesc:a8eaa62eca1d1b02b091befeaa854b4e8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type of experience replay buffer being used. <br /></td></tr>
<tr class="separator:a8eaa62eca1d1b02b091befeaa854b4e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa09f06ba76e978d3571ede38ce61209e"><td class="memItemLeft" align="right" valign="top"><a id="aa09f06ba76e978d3571ede38ce61209e" name="aa09f06ba76e978d3571ede38ce61209e"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>loss_type</b></td></tr>
<tr class="memdesc:aa09f06ba76e978d3571ede38ce61209e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type of loss function used for training. <br /></td></tr>
<tr class="separator:aa09f06ba76e978d3571ede38ce61209e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac0794239af64612da5c0ae9a1b68e18b"><td class="memItemLeft" align="right" valign="top"><a id="ac0794239af64612da5c0ae9a1b68e18b" name="ac0794239af64612da5c0ae9a1b68e18b"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>network_type</b></td></tr>
<tr class="memdesc:ac0794239af64612da5c0ae9a1b68e18b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Type of neural network architecture used. <br /></td></tr>
<tr class="separator:ac0794239af64612da5c0ae9a1b68e18b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a019621ab0c19444789434487859875f5"><td class="memItemLeft" align="right" valign="top"><a id="a019621ab0c19444789434487859875f5" name="a019621ab0c19444789434487859875f5"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>epsilon_schedule</b></td></tr>
<tr class="memdesc:a019621ab0c19444789434487859875f5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Schedule for the exploration parameter epsilon. <br /></td></tr>
<tr class="separator:a019621ab0c19444789434487859875f5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9eb06476c0b741badf76432c43cf2ea7"><td class="memItemLeft" align="right" valign="top"><a id="a9eb06476c0b741badf76432c43cf2ea7" name="a9eb06476c0b741badf76432c43cf2ea7"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>epsilon</b></td></tr>
<tr class="memdesc:a9eb06476c0b741badf76432c43cf2ea7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Scheduler for the exploration parameter epsilon. <br /></td></tr>
<tr class="separator:a9eb06476c0b741badf76432c43cf2ea7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7a9631f575f0ebb622e226bbd9182969"><td class="memItemLeft" align="right" valign="top"><a id="a7a9631f575f0ebb622e226bbd9182969" name="a7a9631f575f0ebb622e226bbd9182969"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>loss</b></td></tr>
<tr class="memdesc:a7a9631f575f0ebb622e226bbd9182969"><td class="mdescLeft">&#160;</td><td class="mdescRight">Loss function used for computing gradients. <br /></td></tr>
<tr class="separator:a7a9631f575f0ebb622e226bbd9182969"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a69a48926f82f3db5a5ba3eefd4c41021"><td class="memItemLeft" align="right" valign="top"><a id="a69a48926f82f3db5a5ba3eefd4c41021" name="a69a48926f82f3db5a5ba3eefd4c41021"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>value_net</b></td></tr>
<tr class="memdesc:a69a48926f82f3db5a5ba3eefd4c41021"><td class="mdescLeft">&#160;</td><td class="mdescRight">The value network that approximates the Q-value function. <br /></td></tr>
<tr class="separator:a69a48926f82f3db5a5ba3eefd4c41021"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4d3529d1a839eacdf67ddbc7717c254"><td class="memItemLeft" align="right" valign="top"><a id="ab4d3529d1a839eacdf67ddbc7717c254" name="ab4d3529d1a839eacdf67ddbc7717c254"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>target_net</b></td></tr>
<tr class="memdesc:ab4d3529d1a839eacdf67ddbc7717c254"><td class="mdescLeft">&#160;</td><td class="mdescRight">The target network, which is a copy of the value network synchronized periodically. <br /></td></tr>
<tr class="separator:ab4d3529d1a839eacdf67ddbc7717c254"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af757bc63e240c7b7b8ef0bfea7257a55"><td class="memItemLeft" align="right" valign="top"><a id="af757bc63e240c7b7b8ef0bfea7257a55" name="af757bc63e240c7b7b8ef0bfea7257a55"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>optimizer</b></td></tr>
<tr class="memdesc:af757bc63e240c7b7b8ef0bfea7257a55"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adam optimizer for training the value network. <br /></td></tr>
<tr class="separator:af757bc63e240c7b7b8ef0bfea7257a55"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18261ebe1ac3a6e98d3bc2883545154c"><td class="memItemLeft" align="right" valign="top"><a id="a18261ebe1ac3a6e98d3bc2883545154c" name="a18261ebe1ac3a6e98d3bc2883545154c"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>buffer</b></td></tr>
<tr class="memdesc:a18261ebe1ac3a6e98d3bc2883545154c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Experience replay buffer for storing transitions. <br /></td></tr>
<tr class="separator:a18261ebe1ac3a6e98d3bc2883545154c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface')"><img src="closed.png" alt="-"/>&#160;Public Attributes inherited from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html">relab.agents.AgentInterface.AgentInterface</a></td></tr>
<tr class="memitem:aebe9410ff0c783d702877cba754bdfe4 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="aebe9410ff0c783d702877cba754bdfe4" name="aebe9410ff0c783d702877cba754bdfe4"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>training</b></td></tr>
<tr class="memdesc:aebe9410ff0c783d702877cba754bdfe4 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flag indicating whether the agent is in training mode. <br /></td></tr>
<tr class="separator:aebe9410ff0c783d702877cba754bdfe4 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af9265d05f4810ac9f1e8a7b81fa2b3e4 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="af9265d05f4810ac9f1e8a7b81fa2b3e4" name="af9265d05f4810ac9f1e8a7b81fa2b3e4"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>device</b></td></tr>
<tr class="memdesc:af9265d05f4810ac9f1e8a7b81fa2b3e4 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">The device (CPU/GPU) used for training computations. <br /></td></tr>
<tr class="separator:af9265d05f4810ac9f1e8a7b81fa2b3e4 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2893cb65b44407a56a2063e76f46b084 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a2893cb65b44407a56a2063e76f46b084" name="a2893cb65b44407a56a2063e76f46b084"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>max_queue_len</b></td></tr>
<tr class="memdesc:a2893cb65b44407a56a2063e76f46b084 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Maximum length for metric tracking queues (e.g., rewards, losses). <br /></td></tr>
<tr class="separator:a2893cb65b44407a56a2063e76f46b084 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a57c5cb4f2b2a27f6f493dc2cb35204de inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a57c5cb4f2b2a27f6f493dc2cb35204de" name="a57c5cb4f2b2a27f6f493dc2cb35204de"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>current_step</b></td></tr>
<tr class="memdesc:a57c5cb4f2b2a27f6f493dc2cb35204de inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Counter tracking the number of training steps performed. <br /></td></tr>
<tr class="separator:a57c5cb4f2b2a27f6f493dc2cb35204de inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ada135459aeaa08b56654573f26d9d164 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="ada135459aeaa08b56654573f26d9d164" name="ada135459aeaa08b56654573f26d9d164"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>vfe_losses</b></td></tr>
<tr class="memdesc:ada135459aeaa08b56654573f26d9d164 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue containing the last variational free energy loss values. <br /></td></tr>
<tr class="separator:ada135459aeaa08b56654573f26d9d164 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4918c5297a7d20739113e60d376d54b0 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a4918c5297a7d20739113e60d376d54b0" name="a4918c5297a7d20739113e60d376d54b0"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>betas</b></td></tr>
<tr class="memdesc:a4918c5297a7d20739113e60d376d54b0 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue containing the last beta values for variational inference. <br /></td></tr>
<tr class="separator:a4918c5297a7d20739113e60d376d54b0 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a228b708b6845c61ae7fb1b263820e972 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a228b708b6845c61ae7fb1b263820e972" name="a228b708b6845c61ae7fb1b263820e972"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>log_likelihoods</b></td></tr>
<tr class="memdesc:a228b708b6845c61ae7fb1b263820e972 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue containing the last log-likelihood values. <br /></td></tr>
<tr class="separator:a228b708b6845c61ae7fb1b263820e972 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a50846610d0bef5788babfada7b23bd7c inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a50846610d0bef5788babfada7b23bd7c" name="a50846610d0bef5788babfada7b23bd7c"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>kl_divergences</b></td></tr>
<tr class="memdesc:a50846610d0bef5788babfada7b23bd7c inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue containing the last KL-divergence values. <br /></td></tr>
<tr class="separator:a50846610d0bef5788babfada7b23bd7c inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afd45587d464b4589f99dffc60c91c86d inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="afd45587d464b4589f99dffc60c91c86d" name="afd45587d464b4589f99dffc60c91c86d"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>process</b></td></tr>
<tr class="memdesc:afd45587d464b4589f99dffc60c91c86d inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Object representing the current process, used to track memory usage. <br /></td></tr>
<tr class="separator:afd45587d464b4589f99dffc60c91c86d inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a01dbb8f53af242be4950b87001457051 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a01dbb8f53af242be4950b87001457051" name="a01dbb8f53af242be4950b87001457051"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>virtual_memory</b></td></tr>
<tr class="memdesc:a01dbb8f53af242be4950b87001457051 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue tracking virtual memory usage over time. <br /></td></tr>
<tr class="separator:a01dbb8f53af242be4950b87001457051 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afc8a86652d557c030960e658bcffc7bf inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="afc8a86652d557c030960e658bcffc7bf" name="afc8a86652d557c030960e658bcffc7bf"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>residential_memory</b></td></tr>
<tr class="memdesc:afc8a86652d557c030960e658bcffc7bf inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue tracking residential memory usage over time. <br /></td></tr>
<tr class="separator:afc8a86652d557c030960e658bcffc7bf inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5391a5dc70168153d9f2870c77c08817 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a5391a5dc70168153d9f2870c77c08817" name="a5391a5dc70168153d9f2870c77c08817"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>episodic_rewards</b></td></tr>
<tr class="memdesc:a5391a5dc70168153d9f2870c77c08817 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue containing the last episodic reward values. <br /></td></tr>
<tr class="separator:a5391a5dc70168153d9f2870c77c08817 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4a9667415d86d25f96a2b24192df7b0 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="ab4a9667415d86d25f96a2b24192df7b0" name="ab4a9667415d86d25f96a2b24192df7b0"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>current_episodic_reward</b></td></tr>
<tr class="memdesc:ab4a9667415d86d25f96a2b24192df7b0 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Accumulator for the current episode's reward. <br /></td></tr>
<tr class="separator:ab4a9667415d86d25f96a2b24192df7b0 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:add513abea2b924b9ff01ea780e615226 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="add513abea2b924b9ff01ea780e615226" name="add513abea2b924b9ff01ea780e615226"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>time_elapsed</b></td></tr>
<tr class="memdesc:add513abea2b924b9ff01ea780e615226 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue containing the time elapsed between consecutive training iterations. <br /></td></tr>
<tr class="separator:add513abea2b924b9ff01ea780e615226 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3946d85e422e39dfb53bdcf0bdb157e2 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a3946d85e422e39dfb53bdcf0bdb157e2" name="a3946d85e422e39dfb53bdcf0bdb157e2"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>last_time</b></td></tr>
<tr class="memdesc:a3946d85e422e39dfb53bdcf0bdb157e2 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Timestamp of the last training iteration. <br /></td></tr>
<tr class="separator:a3946d85e422e39dfb53bdcf0bdb157e2 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9e85b5ccd071a9bd1b31108af768b7f inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="ab9e85b5ccd071a9bd1b31108af768b7f" name="ab9e85b5ccd071a9bd1b31108af768b7f"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>episode_lengths</b></td></tr>
<tr class="memdesc:ab9e85b5ccd071a9bd1b31108af768b7f inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Queue containing the lengths of recent episodes. <br /></td></tr>
<tr class="separator:ab9e85b5ccd071a9bd1b31108af768b7f inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a57c7c72ae83d068259e19c2546fd7db9 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a57c7c72ae83d068259e19c2546fd7db9" name="a57c7c72ae83d068259e19c2546fd7db9"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>current_episode_length</b></td></tr>
<tr class="memdesc:a57c7c72ae83d068259e19c2546fd7db9 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Counter for the current episode's length. <br /></td></tr>
<tr class="separator:a57c7c72ae83d068259e19c2546fd7db9 inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ee2c0fa4c30526206bd7d02cfe321ce inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top"><a id="a7ee2c0fa4c30526206bd7d02cfe321ce" name="a7ee2c0fa4c30526206bd7d02cfe321ce"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>writer</b></td></tr>
<tr class="memdesc:a7ee2c0fa4c30526206bd7d02cfe321ce inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">TensorBoard summary writer for logging training metrics. <br /></td></tr>
<tr class="separator:a7ee2c0fa4c30526206bd7d02cfe321ce inherit pub_attribs_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="inherited" name="inherited"></a>
Additional Inherited Members</h2></td></tr>
<tr class="inherit_header pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td colspan="2" onclick="javascript:toggleInherit('pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface')"><img src="closed.png" alt="-"/>&#160;Static Public Member Functions inherited from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html">relab.agents.AgentInterface.AgentInterface</a></td></tr>
<tr class="memitem:ac2a05fd6065be94e8f17e6626f8badae inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">Optional[str]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#ac2a05fd6065be94e8f17e6626f8badae">get_latest_checkpoint</a> (str regex=r&quot;model_\d+.pt&quot;)</td></tr>
<tr class="memdesc:ac2a05fd6065be94e8f17e6626f8badae inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the latest checkpoint file matching the regex.  <br /></td></tr>
<tr class="separator:ac2a05fd6065be94e8f17e6626f8badae inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af346001fffd195e37ff78eb7ffc73ac6 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">Callable&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#af346001fffd195e37ff78eb7ffc73ac6">get_replay_buffer</a> (<a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1ReplayType.html">ReplayType</a> replay_type, float omega, float omega_is, int n_steps, float gamma=1.0)</td></tr>
<tr class="memdesc:af346001fffd195e37ff78eb7ffc73ac6 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Retrieve the constructor of the replay buffer requested as parameters.  <br /></td></tr>
<tr class="separator:af346001fffd195e37ff78eb7ffc73ac6 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a704603931adc5148ee15311a72056935 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">Any&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a704603931adc5148ee15311a72056935">safe_load</a> (Checkpoint checkpoint, str key)</td></tr>
<tr class="memdesc:a704603931adc5148ee15311a72056935 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load the value corresponding to the key in the checkpoint.  <br /></td></tr>
<tr class="separator:a704603931adc5148ee15311a72056935 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a74a42b5a97d74fb792cb43455d274a22 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memItemLeft" align="right" valign="top">Optimizer&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a74a42b5a97d74fb792cb43455d274a22">safe_load_optimizer</a> (Checkpoint checkpoint, Union[Iterator[Parameter], List[Parameter]] params, float learning_rate, float adam_eps)</td></tr>
<tr class="memdesc:a74a42b5a97d74fb792cb43455d274a22 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="mdescLeft">&#160;</td><td class="mdescRight">Load the Adam optimizer from the checkpoint safely.  <br /></td></tr>
<tr class="separator:a74a42b5a97d74fb792cb43455d274a22 inherit pub_static_methods_classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Implements a Deep Q-Network. </p>
<p>This implementation is based on the paper:</p>
<p><b>Human-level control through deep reinforcement learning</b>, published in Nature, 2015.</p>
<p>Authors:</p><ul>
<li>Volodymyr Mnih</li>
<li>Koray Kavukcuoglu</li>
<li>David Silver</li>
<li>Andrei A. Rusu</li>
<li>Joel Veness</li>
<li>Marc G. Bellemare</li>
<li>Alex Graves</li>
<li>Martin Riedmiller</li>
<li>Andreas K. Fidjeland</li>
<li>Georg Ostrovski, et al.</li>
</ul>
<p>The paper introduced the DQN algorithm, combining Q-learning with deep neural networks to achieve human-level performance in Atari 2600 games. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a2d91aced72b5b4b01098fba9e85abaf3" name="a2d91aced72b5b4b01098fba9e85abaf3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2d91aced72b5b4b01098fba9e85abaf3">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None relab.agents.DQN.DQN.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>gamma</em> = <code>0.99</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>learning_rate</em> = <code>0.00001</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int  &#160;</td>
          <td class="paramname"><em>buffer_size</em> = <code>1000000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>batch_size</em> = <code>32</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>learning_starts</em> = <code>200000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>kappa</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>target_update_interval</em> = <code>40000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>adam_eps</em> = <code>1.5e-4</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>n_actions</em> = <code>18</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[int] &#160;</td>
          <td class="paramname"><em>n_atoms</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>v_min</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>v_max</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>n_steps</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>training</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1ReplayType.html">ReplayType</a> &#160;</td>
          <td class="paramname"><em>replay_type</em> = <code><a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1ReplayType.html#a84ebd587efe64396ce0fc381e39406e7">ReplayType.DEFAULT</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1LossType.html">LossType</a> &#160;</td>
          <td class="paramname"><em>loss_type</em> = <code><a class="el" href="classrelab_1_1agents_1_1DQN_1_1LossType.html#a7fc4dd09dd09f4cad7e8b2f80525794f">LossType.DQN_SL1</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1NetworkType.html">NetworkType</a> &#160;</td>
          <td class="paramname"><em>network_type</em> = <code><a class="el" href="classrelab_1_1agents_1_1DQN_1_1NetworkType.html#a151e691483dc02332cb23e2d3d9c88a8">NetworkType.DEFAULT</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>omega</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>omega_is</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Any &#160;</td>
          <td class="paramname"><em>epsilon_schedule</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Create a DQN agent. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">gamma</td><td>the discount factor </td></tr>
    <tr><td class="paramname">learning_rate</td><td>the learning rate </td></tr>
    <tr><td class="paramname">buffer_size</td><td>the size of the replay buffer </td></tr>
    <tr><td class="paramname">batch_size</td><td>the size of the batches sampled from the replay buffer </td></tr>
    <tr><td class="paramname">learning_starts</td><td>the step at which learning starts </td></tr>
    <tr><td class="paramname">kappa</td><td>the kappa parameter of the quantile Huber loss see Equation (10) in QR-DQN paper </td></tr>
    <tr><td class="paramname">target_update_interval</td><td>number of training steps between two synchronization of the target </td></tr>
    <tr><td class="paramname">adam_eps</td><td>the epsilon parameter of the Adam optimizer </td></tr>
    <tr><td class="paramname">n_actions</td><td>the number of actions available to the agent </td></tr>
    <tr><td class="paramname">n_atoms</td><td>the number of atoms used to approximate the distribution over returns </td></tr>
    <tr><td class="paramname">v_min</td><td>the minimum amount of returns (only used for distributional DQN) </td></tr>
    <tr><td class="paramname">v_max</td><td>the maximum amount of returns (only used for distributional DQN) </td></tr>
    <tr><td class="paramname">training</td><td>True if the agent is being trained, False otherwise </td></tr>
    <tr><td class="paramname">n_steps</td><td>the number of steps for which rewards are accumulated in multistep Q-learning </td></tr>
    <tr><td class="paramname">omega</td><td>the prioritization exponent </td></tr>
    <tr><td class="paramname">omega_is</td><td>the important sampling exponent </td></tr>
    <tr><td class="paramname">replay_type</td><td>the type of replay buffer </td></tr>
    <tr><td class="paramname">loss_type</td><td>the loss to use during gradient descent </td></tr>
    <tr><td class="paramname">network_type</td><td>the network architecture to use for the value and target networks </td></tr>
    <tr><td class="paramname">epsilon_schedule</td><td>the schedule for the exploration parameter epsilon as a list of tuple, i.e., [(step_1, value_1), (step_2, value_2), ..., (step_n, value_n)] </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a37b65628d214627afae8cf7c8f576435">relab.agents.AgentInterface.AgentInterface</a>.</p>

<p>Reimplemented in <a class="el" href="classrelab_1_1agents_1_1QRDQN_1_1QRDQN.html#aac3f9b94fe8b45ea63035c42ebdca214">relab.agents.QRDQN.QRDQN</a>, <a class="el" href="classrelab_1_1agents_1_1RainbowIQN_1_1RainbowIQN.html#a3262211e23dc1181ae30580740beb2b9">relab.agents.RainbowIQN.RainbowIQN</a>, <a class="el" href="classrelab_1_1agents_1_1IQN_1_1IQN.html#a744f1a08268e137f79f3e6e6baafa32d">relab.agents.IQN.IQN</a>, <a class="el" href="classrelab_1_1agents_1_1PrioritizedMDQN_1_1PrioritizedMDQN.html#a1d880f1cfffb16b9e29d87fe39e753c2">relab.agents.PrioritizedMDQN.PrioritizedMDQN</a>, <a class="el" href="classrelab_1_1agents_1_1MDQN_1_1MDQN.html#a8a4b112cc8177e91233bd1c5d090e008">relab.agents.MDQN.MDQN</a>, <a class="el" href="classrelab_1_1agents_1_1NoisyCDQN_1_1NoisyCDQN.html#a6958dcd69e1a355c07f7f85c23362338">relab.agents.NoisyCDQN.NoisyCDQN</a>, <a class="el" href="classrelab_1_1agents_1_1CDQN_1_1CDQN.html#a4bb9953c5b41bfaaa97b121424775214">relab.agents.CDQN.CDQN</a>, <a class="el" href="classrelab_1_1agents_1_1DDQN_1_1DDQN.html#a78152f0e1c6a5e5f1d0eda5fb92c37e0">relab.agents.DDQN.DDQN</a>, <a class="el" href="classrelab_1_1agents_1_1DuelingDDQN_1_1DuelingDDQN.html#a4cd4c3c9daf4a504ae725e9b68ed04fe">relab.agents.DuelingDDQN.DuelingDDQN</a>, <a class="el" href="classrelab_1_1agents_1_1NoisyDDQN_1_1NoisyDDQN.html#a13fbd3d92d5641d5a454974c0894cbec">relab.agents.NoisyDDQN.NoisyDDQN</a>, <a class="el" href="classrelab_1_1agents_1_1DuelingDQN_1_1DuelingDQN.html#ab8ca0aa540e382510474fc852fa88993">relab.agents.DuelingDQN.DuelingDQN</a>, <a class="el" href="classrelab_1_1agents_1_1NoisyDQN_1_1NoisyDQN.html#ad7e902ae7cf4bbf5bb69df947be940f0">relab.agents.NoisyDQN.NoisyDQN</a>, <a class="el" href="classrelab_1_1agents_1_1RainbowDQN_1_1RainbowDQN.html#a2b2d06a182d120ed06ea1bc7b2ddb3e3">relab.agents.RainbowDQN.RainbowDQN</a>, <a class="el" href="classrelab_1_1agents_1_1PrioritizedDDQN_1_1PrioritizedDDQN.html#a0ee8d33986b8ee610e32e8f81e5f52e6">relab.agents.PrioritizedDDQN.PrioritizedDDQN</a>, and <a class="el" href="classrelab_1_1agents_1_1PrioritizedDQN_1_1PrioritizedDQN.html#a4b0ec0729b782e8673397337d6c4cbda">relab.agents.PrioritizedDQN.PrioritizedDQN</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="afaea756d733758ffbba110e0ebea7cdf" name="afaea756d733758ffbba110e0ebea7cdf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afaea756d733758ffbba110e0ebea7cdf">&#9670;&#160;</a></span>as_dict()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def relab.agents.DQN.DQN.as_dict </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">"
Convert the agent into a dictionary that can be saved on the filesystem.
@return the dictionary
</pre> 
<p>Reimplemented from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a4725a741348861a5c9f9395df508f063">relab.agents.AgentInterface.AgentInterface</a>.</p>

</div>
</div>
<a id="a1bb2e42715c44dbef27fc53214e46bdf" name="a1bb2e42715c44dbef27fc53214e46bdf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1bb2e42715c44dbef27fc53214e46bdf">&#9670;&#160;</a></span>categorical_kl_divergence()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tensor relab.agents.DQN.DQN.categorical_kl_divergence </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>rewards</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor
    &#160;</td>
          <td class="paramname"><em>next_obs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the loss of the categorical algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">obs</td><td>the observations at time t </td></tr>
    <tr><td class="paramname">actions</td><td>the actions at time t </td></tr>
    <tr><td class="paramname">rewards</td><td>the reward obtained when taking the actions while seeing the observations at time t </td></tr>
    <tr><td class="paramname">done</td><td>whether the episodes ended </td></tr>
    <tr><td class="paramname">next_obs</td><td>the observation at time t + 1 </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the categorical loss </dd></dl>

</div>
</div>
<a id="aa493fd9606ce72d1bad97b67f6f73110" name="aa493fd9606ce72d1bad97b67f6f73110"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa493fd9606ce72d1bad97b67f6f73110">&#9670;&#160;</a></span>get_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Callable relab.agents.DQN.DQN.get_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1LossType.html">LossType</a>&#160;</td>
          <td class="paramname"><em>loss_type</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Retrieve the loss requested as parameters. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">loss_type</td><td>the loss to use during gradient descent </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the loss </dd></dl>

</div>
</div>
<a id="a5bcef6e093e941977ba2716ab7469df5" name="a5bcef6e093e941977ba2716ab7469df5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5bcef6e093e941977ba2716ab7469df5">&#9670;&#160;</a></span>get_value_network()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> nn.Module relab.agents.DQN.DQN.get_value_network </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1NetworkType.html">NetworkType</a>&#160;</td>
          <td class="paramname"><em>network_type</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Retrieve the constructor of the value network requested as parameters. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">network_type</td><td>the network architecture to use for the value and target networks </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the constructor of the value network </dd></dl>

</div>
</div>
<a id="a1342607ce111ebf505f15cd68d5cdeeb" name="a1342607ce111ebf505f15cd68d5cdeeb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1342607ce111ebf505f15cd68d5cdeeb">&#9670;&#160;</a></span>implicit_quantile_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tensor relab.agents.DQN.DQN.implicit_quantile_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>rewards</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>next_obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>kappa</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the loss of the quantile regression algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">obs</td><td>the observations at time t </td></tr>
    <tr><td class="paramname">actions</td><td>the actions at time t </td></tr>
    <tr><td class="paramname">rewards</td><td>the reward obtained when taking the actions while seeing the observations at time t </td></tr>
    <tr><td class="paramname">done</td><td>whether the episodes ended </td></tr>
    <tr><td class="paramname">next_obs</td><td>the observation at time t + 1 </td></tr>
    <tr><td class="paramname">kappa</td><td>the kappa parameter of the quantile Huber loss see Equation (10) in QR-DQN paper </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the categorical loss </dd></dl>

</div>
</div>
<a id="ab5eba6790ccbd1d07171f0c2a5df2f93" name="ab5eba6790ccbd1d07171f0c2a5df2f93"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab5eba6790ccbd1d07171f0c2a5df2f93">&#9670;&#160;</a></span>load()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[str, Checkpoint] relab.agents.DQN.DQN.load </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>checkpoint_name</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>buffer_checkpoint_name</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Load an agent from the filesystem. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">checkpoint_name</td><td>the name of the agent checkpoint to load </td></tr>
    <tr><td class="paramname">buffer_checkpoint_name</td><td>the name of the replay buffer checkpoint to load (None for default name) </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>a tuple containing the checkpoint path and the checkpoint object </dd></dl>

<p>Reimplemented from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a162eb28fb9a698f2a58908c4a1d6110e">relab.agents.AgentInterface.AgentInterface</a>.</p>

</div>
</div>
<a id="ae48f88cd975789aa822759948024a52d" name="ae48f88cd975789aa822759948024a52d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae48f88cd975789aa822759948024a52d">&#9670;&#160;</a></span>q_learning_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tensor relab.agents.DQN.DQN.q_learning_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>rewards</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>next_obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Loss&#160;</td>
          <td class="paramname"><em>loss_fc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>double_ql</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the loss of the standard or double Q-learning algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">obs</td><td>the observations at time t </td></tr>
    <tr><td class="paramname">actions</td><td>the actions at time t </td></tr>
    <tr><td class="paramname">rewards</td><td>the reward obtained when taking the actions while seeing the observations at time t </td></tr>
    <tr><td class="paramname">done</td><td>whether the episodes ended </td></tr>
    <tr><td class="paramname">next_obs</td><td>the observation at time t + 1 </td></tr>
    <tr><td class="paramname">loss_fc</td><td>the loss function to use to compare target and prediction </td></tr>
    <tr><td class="paramname">double_ql</td><td>False for standard Q-learning, True for Double Q-learning </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the Q-value loss </dd></dl>

</div>
</div>
<a id="ad23433148a895775990e6dc59622fd73" name="ad23433148a895775990e6dc59622fd73"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad23433148a895775990e6dc59622fd73">&#9670;&#160;</a></span>quantile_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tensor relab.agents.DQN.DQN.quantile_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>rewards</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>next_obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>kappa</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the loss of the quantile regression algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">obs</td><td>the observations at time t </td></tr>
    <tr><td class="paramname">actions</td><td>the actions at time t </td></tr>
    <tr><td class="paramname">rewards</td><td>the reward obtained when taking the actions while seeing the observations at time t </td></tr>
    <tr><td class="paramname">done</td><td>whether the episodes ended </td></tr>
    <tr><td class="paramname">next_obs</td><td>the observation at time t + 1 </td></tr>
    <tr><td class="paramname">kappa</td><td>the kappa parameter of the quantile Huber loss see Equation (10) in QR-DQN paper </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the categorical loss </dd></dl>

</div>
</div>
<a id="a6e8746bfe529654d438c361485e32291" name="a6e8746bfe529654d438c361485e32291"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6e8746bfe529654d438c361485e32291">&#9670;&#160;</a></span>rainbow_iqn_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tensor relab.agents.DQN.DQN.rainbow_iqn_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>rewards</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>next_obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>kappa</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the loss of the rainbow IQN. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">obs</td><td>the observations at time t </td></tr>
    <tr><td class="paramname">actions</td><td>the actions at time t </td></tr>
    <tr><td class="paramname">rewards</td><td>the reward obtained when taking the actions while seeing the observations at time t </td></tr>
    <tr><td class="paramname">done</td><td>whether the episodes ended </td></tr>
    <tr><td class="paramname">next_obs</td><td>the observation at time t + 1 </td></tr>
    <tr><td class="paramname">kappa</td><td>the kappa parameter of the quantile Huber loss see Equation (3) in IQN paper </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the rainbow IQN loss </dd></dl>

</div>
</div>
<a id="a6a59bd7deacfd2b50b3bddd6b1593b44" name="a6a59bd7deacfd2b50b3bddd6b1593b44"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6a59bd7deacfd2b50b3bddd6b1593b44">&#9670;&#160;</a></span>rainbow_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tensor relab.agents.DQN.DQN.rainbow_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>actions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>rewards</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor&#160;</td>
          <td class="paramname"><em>done</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor
    &#160;</td>
          <td class="paramname"><em>next_obs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the loss of the rainbow DQN. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">obs</td><td>the observations at time t </td></tr>
    <tr><td class="paramname">actions</td><td>the actions at time t </td></tr>
    <tr><td class="paramname">rewards</td><td>the reward obtained when taking the actions while seeing the observations at time t </td></tr>
    <tr><td class="paramname">done</td><td>whether the episodes ended </td></tr>
    <tr><td class="paramname">next_obs</td><td>the observation at time t + 1 </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the rainbow loss </dd></dl>

</div>
</div>
<a id="a90e9b39f00d5c50fcdb6b506c9fd99fd" name="a90e9b39f00d5c50fcdb6b506c9fd99fd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a90e9b39f00d5c50fcdb6b506c9fd99fd">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None relab.agents.DQN.DQN.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>checkpoint_name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>buffer_checkpoint_name</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Save the agent on the filesystem. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">checkpoint_name</td><td>the name of the checkpoint in which to save the agent </td></tr>
    <tr><td class="paramname">buffer_checkpoint_name</td><td>the name of the checkpoint to save the replay buffer (None for default name) </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a19be8462f89a85eb5c6f4dc9c07201f9">relab.agents.AgentInterface.AgentInterface</a>.</p>

</div>
</div>
<a id="a84fd3832544a0135720516681e4fff3b" name="a84fd3832544a0135720516681e4fff3b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a84fd3832544a0135720516681e4fff3b">&#9670;&#160;</a></span>step()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> ActionType relab.agents.DQN.DQN.step </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">ObservationType&#160;</td>
          <td class="paramname"><em>obs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Select the next action to perform in the environment. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">obs</td><td>the observation available to make the decision </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the next action to perform </dd></dl>

<p>Reimplemented from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#a5dc04ff5e1441ba6216f2f24e8d8c632">relab.agents.AgentInterface.AgentInterface</a>.</p>

</div>
</div>
<a id="afb3c1f57ed16d94b8cd82bf4ffe24276" name="afb3c1f57ed16d94b8cd82bf4ffe24276"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afb3c1f57ed16d94b8cd82bf4ffe24276">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None relab.agents.DQN.DQN.train </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Env&#160;</td>
          <td class="paramname"><em>env</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Train the agent in the gym environment passed as parameters. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">env</td><td>the gym environment </td></tr>
  </table>
  </dd>
</dl>

<p>Reimplemented from <a class="el" href="classrelab_1_1agents_1_1AgentInterface_1_1AgentInterface.html#acb6aee67e6278cd5230ab2808f774577">relab.agents.AgentInterface.AgentInterface</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>relab/agents/DQN.py</li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><b>relab</b></li><li class="navelem"><b>agents</b></li><li class="navelem"><b>DQN</b></li><li class="navelem"><a class="el" href="classrelab_1_1agents_1_1DQN_1_1DQN.html">DQN</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.6 </li>
  </ul>
</div>
</body>
</html>
