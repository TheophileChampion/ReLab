<!-- HTML header for doxygen 1.9.8-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.6"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ReLab | Documentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🚀</text></svg>">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="doxygen-awesome-darkmode-toggle.js"></script>
<script type="text/javascript">
    DoxygenAwesomeDarkModeToggle.init()
</script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only-darkmode-toggle.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="relab-logo-only.png"/></td>
  <td id="projectalign">
   <div id="projectname">ReLab<span id="projectnumber">&#160;v1.0.0-b</span>
   </div>
   <div id="projectbrief">Reinforcement Learning Benchmarks</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.6 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('md_Tutorial.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">📗 Tutorial </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>ReLab is a flexible and powerful library for training, evaluating, and analyzing reinforcement learning agents. This tutorial will guide you through its core features, from configuring environments and creating agents to running complete experiments using ReLab's python API.</p>
<h2><a class="anchor" id="autotoc_md1"></a>
1. Understanding the Data Directory Structure</h2>
<p>When running ReLab scripts, the library organizes all generated files into a <code>data</code> directory. This structured directory ensures that your experiment outputs are logically grouped, making it easy to access and analyze the results. Below is an overview of the <code>data</code> directory and its purpose:</p>
<div class="fragment"><div class="line">data/</div>
<div class="line">├── demos</div>
<div class="line">│   └── &lt;Environment&gt;</div>
<div class="line">│        └── &lt;Agent&gt;</div>
<div class="line">│             └── &lt;Seed&gt;</div>
<div class="line">│                  └── demo_&lt;iteration&gt;.gif</div>
<div class="line">├── graphs</div>
<div class="line">│   └── &lt;Environment&gt;</div>
<div class="line">│        ├── &lt;Metric&gt;.pdf</div>
<div class="line">│        └── &lt;Agent&gt;</div>
<div class="line">│             └── &lt;Metric&gt;.tsv</div>
<div class="line">├── runs</div>
<div class="line">│   └── &lt;Environment&gt;</div>
<div class="line">│        └── &lt;Agent&gt;</div>
<div class="line">│             └── &lt;Seed&gt;</div>
<div class="line">│                  └── events.out.tfevents.&lt;timestamp&gt;.&lt;hostname&gt;.&lt;PID&gt;.&lt;UID&gt;</div>
<div class="line">└── saves</div>
<div class="line">    └── &lt;Environment&gt;</div>
<div class="line">         └── &lt;Agent&gt;</div>
<div class="line">              └── &lt;Seed&gt;</div>
<div class="line">                   ├── buffer.pt</div>
<div class="line">                   └── model_&lt;iteration&gt;.pt</div>
</div><!-- fragment --><p>Here’s what each folder contains:</p>
<ol type="1">
<li><code>demos/</code>: <br  />
 This folder contains GIFs demonstrating the agent's learned policy. <br  />
<ul>
<li>For each environment, agent, and random seed, ReLab generates GIFs representing specific training iterations. <br  />
</li>
<li>Example: <code>demo_500000.gif</code> shows the agent's behavior after 500,000 training iterations.</li>
</ul>
</li>
<li><code>graphs/</code>: <br  />
 This folder contains visualizations of agent performance metrics. <br  />
<ul>
<li>Metric graphs (e.g., <code>mean_episodic_reward.pdf</code>) are stored for each environment and summarize the performance of one or more agents. <br  />
</li>
<li>Detailed data files (e.g., <code>mean_episodic_reward.tsv</code>) are also stored here for individual agents, containing the mean and standard deviation of the specified metric at each training step.</li>
</ul>
</li>
<li><code>runs/</code>: <br  />
 This folder logs training data in a format compatible with <a href="https://www.tensorflow.org/tensorboard">TensorBoard</a>. <br  />
<ul>
<li>Each environment-agent-seed combination has its own folder containing event files (e.g., <code>events.out.tfevents...</code>) that allow you to track the agent’s progress during training.</li>
</ul>
</li>
<li><code>saves/</code>: <br  />
 This folder stores the saved models for each training session. <br  />
<ul>
<li>Model checkpoints are saved for specific training iterations (e.g., <code>model_500000.pt</code>), allowing you to reload and evaluate the agent at different stages of training.</li>
<li>Replay buffer checkpoint (e.g., <code>buffer.pt</code>) saves the replay buffer associated with the last checkpoint iteration, ensuring training can resume seamlessly from where it was left off. For example, if the directory contains <code>model_500.pt</code> and <code>model_1000.pt</code>, then <code>buffer.pt</code> corresponds to the replay buffer at iteration 1000.</li>
</ul>
</li>
</ol>
<p>By organizing experiment outputs in this way, ReLab ensures that your data is easy to locate and manage, enabling you to efficiently analyze results, compare agents, and showcase their learned behaviors.</p>
<h2><a class="anchor" id="autotoc_md2"></a>
2. ReLab Configuration and Initialization</h2>
<p>ReLab's configuration allows you to customize key aspects of training and logging. Here are the most relevant entries:</p>
<ul>
<li><code>max_n_steps</code>: Maximum number of training iterations (default: 50,000,000). <br  />
 Defines the iterations at which training is stopped.</li>
<li><code>checkpoint_frequency</code>: Number of training iterations between model checkpoints (default: 500,000). <br  />
 Checkpoints save the agent's state, enabling you to resume training or analyze the agent progress.</li>
<li><code>tensorboard_log_interval</code>: Number of training iterations between TensorBoard log updates (default: 5,000). <br  />
 Controls how frequently training metrics (e.g., rewards) are logged for visualization.</li>
<li><code>save_all_replay_buffers</code>: Determines whether all replay buffers are saved (default: <code>False</code>). <br  />
 If <code>False</code>, only the replay buffer associated with the most recent checkpoint is saved.</li>
</ul>
<h4><a class="anchor" id="autotoc_md3"></a>
Example Usage</h4>
<div class="fragment"><div class="line"><span class="comment"># Retrieve a specific config value.</span></div>
<div class="line">max_steps = relab.config(<span class="stringliteral">&quot;max_n_steps&quot;</span>)</div>
<div class="line">print(f<span class="stringliteral">&quot;Maximum training steps: {max_steps}&quot;</span>)</div>
</div><!-- fragment --><hr  />
<p>Before doing anything with ReLab, the <code>relab.initialize()</code> function must be called. It is the first step to setting up the library, ensuring that all paths are properly configured. Here's a quick breakdown:</p>
<div class="fragment"><div class="line">relab.initialize(</div>
<div class="line">    agent_name=<span class="stringliteral">&quot;DQN&quot;</span>,          <span class="comment"># Name of the agent (e.g., &quot;DQN&quot;, &quot;RainbowDQN&quot;).</span></div>
<div class="line">    env_name=<span class="stringliteral">&quot;ALE/Pong-v5&quot;</span>,    <span class="comment"># Environment on which the agent will be trained or evaluated.</span></div>
<div class="line">    seed=0,                    <span class="comment"># Random seed for reproducibility.</span></div>
<div class="line">    data_directory=<span class="keywordtype">None</span>,       <span class="comment"># Path for storing all data; defaults to &quot;./data&quot;.</span></div>
<div class="line">    paths_only=<span class="keyword">False</span>           <span class="comment"># If True, initializes paths without setting up the framework.</span></div>
<div class="line">)</div>
</div><!-- fragment --><p>This function performs several key steps: <br  />
</p><ul>
<li>Ensures reproducibility by setting the random seed for NumPy, Python, and PyTorch. <br  />
</li>
<li>Registers additional environments (e.g., Atari games and custom environments) with the Gym framework. <br  />
</li>
<li>Initializes environment variables (e.g., <code>CHECKPOINT_DIRECTORY</code> and <code>TENSORBOARD_DIRECTORY</code>) to define where specific files are stored, ensuring consistency across scripts.</li>
</ul>
<h2><a class="anchor" id="autotoc_md5"></a>
3. Creating Agents</h2>
<p>The <code>relab.agents.make()</code> function is a factory method that simplifies the creation of reinforcement learning agents in ReLab. By passing the name of the desired agent and optional keyword arguments, you can create and configure agents with ease.</p>
<h3><a class="anchor" id="autotoc_md6"></a>
3.1. Function Overview</h3>
<div class="fragment"><div class="line"><span class="keyword">def </span>make(agent_name: str, **kwargs: Any) -&gt; AgentInterface:</div>
</div><!-- fragment --><ul>
<li><code>agent_name</code>: The name of the agent to instantiate. Must be one of the supported agents (listed below). If an unsupported name is provided, the function raises an error. <br  />
</li>
<li><code>kwargs</code>: Keyword arguments forwarded to the agent's constructor, allowing you to customize the agent's behavior.</li>
</ul>
<h4><a class="anchor" id="autotoc_md7"></a>
Example Usage</h4>
<div class="fragment"><div class="line"><span class="keyword">from</span> relab <span class="keyword">import</span> agents</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create a Dueling Double DQN agent.</span></div>
<div class="line">agent = agents.make(<span class="stringliteral">&quot;DuelingDDQN&quot;</span>, learning_rate=0.0001, gamma=0.99)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md8"></a>
3.2. Supported Agents: Overview Table</h3>
<p>Here’s a table summarizing the supported agents in ReLab. It includes their full names, abbreviations, and key characteristics such as whether they are value-based, distributional, random, or learn a world model.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone"><b>Abbreviation</b>   </th><th class="markdownTableHeadNone"><b>Full Name</b>   </th><th class="markdownTableHeadNone"><b>Value-Based</b>   </th><th class="markdownTableHeadNone"><b>Distributional</b>   </th><th class="markdownTableHeadNone"><b>Random Actions</b>   </th><th class="markdownTableHeadNone"><b>World Model</b>    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>DQN</b>   </td><td class="markdownTableBodyNone">Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>DDQN</b>   </td><td class="markdownTableBodyNone">Double Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>CDQN</b>   </td><td class="markdownTableBodyNone">Categorical Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>MDQN</b>   </td><td class="markdownTableBodyNone">Multi-step Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>QRDQN</b>   </td><td class="markdownTableBodyNone">Quantile Regression Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>NoisyDQN</b>   </td><td class="markdownTableBodyNone">Noisy Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️ (noisy layers for exploration)   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>NoisyDDQN</b>   </td><td class="markdownTableBodyNone">Noisy Double Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️ (noisy layers for exploration)   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>NoisyCDQN</b>   </td><td class="markdownTableBodyNone">Noisy Categorical Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️ (noisy layers for exploration)   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>DuelingDQN</b>   </td><td class="markdownTableBodyNone">Dueling Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>DuelingDDQN</b>   </td><td class="markdownTableBodyNone">Dueling Double Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>PrioritizedDQN</b>   </td><td class="markdownTableBodyNone">Prioritized Experience Replay DQN   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>PrioritizedDDQN</b>   </td><td class="markdownTableBodyNone">Prioritized Experience Replay DDQN   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>PrioritizedMDQN</b>   </td><td class="markdownTableBodyNone">Prioritized Multi-step DQN   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>RainbowDQN</b>   </td><td class="markdownTableBodyNone">Rainbow Deep Q-Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>RainbowIQN</b>   </td><td class="markdownTableBodyNone">Rainbow with Implicit Quantile Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>IQN</b>   </td><td class="markdownTableBodyNone">Implicit Quantile Network   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>Random</b>   </td><td class="markdownTableBodyNone">Random Agent   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✖️    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>VAE</b>   </td><td class="markdownTableBodyNone">Variational Autoencoder   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>BetaVAE</b>   </td><td class="markdownTableBodyNone">Beta Variational Autoencoder   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>DiscreteVAE</b>   </td><td class="markdownTableBodyNone">Discrete Variational Autoencoder   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>JointVAE</b>   </td><td class="markdownTableBodyNone">Joint Variational Autoencoder   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>HMM</b>   </td><td class="markdownTableBodyNone">Hidden Markov Model   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>BetaHMM</b>   </td><td class="markdownTableBodyNone">Beta Hidden Markov Model   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>DiscreteHMM</b>   </td><td class="markdownTableBodyNone">Discrete Hidden Markov Model   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><b>JointHMM</b>   </td><td class="markdownTableBodyNone">Joint Hidden Markov Model   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✖️   </td><td class="markdownTableBodyNone">✅   </td><td class="markdownTableBodyNone">✅   </td></tr>
</table>
<h4><a class="anchor" id="autotoc_md9"></a>
Notes:</h4>
<ol type="1">
<li><b>Value-Based Agents</b>: Agents like DQN and DDQN focus on learning a value function to determine optimal actions.</li>
<li><b>Distributional Agents</b>: Distributional RL agents (e.g., QRDQN, CDQN) model the distribution of returns instead of estimating a single expected return.</li>
<li><b>Random Actions</b>: Several agents take random actions, they can be used either to learn a world model or as a baseline for comparing more sophisticated agents.</li>
<li><b>World Model Agents</b>: Agents like VAEs and HMMs focus on learning a representation of the environment or the "world model," which can be used for planning or analysis.</li>
</ol>
<h2><a class="anchor" id="autotoc_md10"></a>
4. Creating Environments</h2>
<p>The <code>relab.environments.make()</code> function is a factory that provides an easy and customizable way to set up Gym environments for training reinforcement learning agents.</p>
<h3><a class="anchor" id="autotoc_md11"></a>
4.1. Function Overview</h3>
<div class="fragment"><div class="line"><span class="keyword">def </span>make(env_name: str, **kwargs: Any) -&gt; Env:</div>
</div><!-- fragment --><ul>
<li><code>env_name</code>: The name of the environment to instantiate.</li>
<li><code>kwargs</code>: Keyword arguments forwarded to the environment's constructor, allowing you to customize the environment.</li>
</ul>
<h4><a class="anchor" id="autotoc_md12"></a>
Note, the function applies several preprocessing steps:</h4>
<ul>
<li><b>Environment Setup</b>: Initializes the environment with <code>gym.make</code>, by default the entire action space is used (18 actions for all Atari games).</li>
<li><b>FireReset Wrapper</b>: Ensures that the environment resets properly by simulating a fire action where applicable.</li>
<li><b>Atari Preprocessing</b>:<ul>
<li>Rescales observations to the configured screen size (<code>screen_size</code>).</li>
<li>Converts observations to grayscale.</li>
<li>Scales pixel values for improved learning stability.</li>
<li>Skips frames as defined in <code>frame_skip</code>.</li>
</ul>
</li>
<li><b>Frame Stacking</b>: Stacks the last <code>stack_size</code> observations to provide temporal context for agents.</li>
<li><b>Torch Integration</b>: Converts environment outputs to PyTorch tensors for seamless agent interaction.</li>
</ul>
<h4><a class="anchor" id="autotoc_md13"></a>
Example Usage</h4>
<div class="fragment"><div class="line"><span class="keyword">from</span> relab <span class="keyword">import</span> environments</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Create an environment running the Atari game pong.</span></div>
<div class="line">env = environments.make(<span class="stringliteral">&quot;ALE/Pong-v5&quot;</span>)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md14"></a>
4.2. Predefined Atari Game Sets</h3>
<p>At times, you might want to evaluate your agents on a specific subset of Atari games. ReLab provides three predefined Atari benchmarks to simplify this process:</p>
<ol type="1">
<li><code>small_benchmark_atari_games()</code><ul>
<li>Returns a small subset of five Atari games for quick benchmarking:<ul>
<li>Breakout</li>
<li>Freeway</li>
<li>Ms. Pac-Man</li>
<li>Pong</li>
<li>Space Invaders</li>
</ul>
</li>
</ul>
</li>
<li><code>benchmark_atari_games()</code><ul>
<li>Returns the standard set of 57 Atari games used in reinforcement learning research benchmarks.</li>
<li>Includes all games from <code>small_benchmark_atari_games()</code> plus additional titles like Asteroids, Seaquest, and Montezuma’s Revenge.</li>
</ul>
</li>
<li><code>all_atari_games()</code><ul>
<li>Returns all available Atari games, including the benchmark games and extra titles like Adventure and Air Raid.</li>
</ul>
</li>
</ol>
<h4><a class="anchor" id="autotoc_md15"></a>
Example Usage:</h4>
<div class="fragment"><div class="line"><span class="keyword">from</span> relab <span class="keyword">import</span> environments</div>
<div class="line"> </div>
<div class="line"><span class="comment"># Retrieve the list of Atari benchmark games.</span></div>
<div class="line">benchmark_games = environments.benchmark_atari_games()</div>
<div class="line">print(f<span class="stringliteral">&quot;Total Atari Benchmark Games: {len(benchmark_games)}&quot;</span>)</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md16"></a>
5. Training your First Agent</h2>
<p>By now, you’ve learned about ReLab's features, how to configure the library, create agents and environments, and manage saved data and benchmarks. Let’s bring it all together with a complete training script to demonstrate how these components work in practice:</p>
<div class="fragment"><div class="line"><span class="keyword">from</span> relab <span class="keyword">import</span> agents</div>
<div class="line"><span class="keyword">import</span> relab</div>
<div class="line"><span class="keyword">from</span> relab <span class="keyword">import</span> environments</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keyword">def </span>run_training(agent : str, env : str, seed : int) -&gt; <span class="keywordtype">None</span>:</div>
<div class="line">    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="stringliteral">    Train a reinforcement learning agent on a gym environment.</span></div>
<div class="line"><span class="stringliteral">    :param agent: the agent name</span></div>
<div class="line"><span class="stringliteral">    :param env: the environment name</span></div>
<div class="line"><span class="stringliteral">    :param seed: the random seed</span></div>
<div class="line"><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">    </span><span class="comment"># Initialize the benchmark.</span></div>
<div class="line">    relab.initialize(agent, env, seed)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Create the environment.</span></div>
<div class="line">    env = environments.make(env)</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Create and train the agent.</span></div>
<div class="line">    agent = agents.make(agent, training=<span class="keyword">True</span>)</div>
<div class="line">    agent.load()</div>
<div class="line">    agent.train(env)</div>
<div class="line"> </div>
<div class="line"> </div>
<div class="line"><span class="keywordflow">if</span> __name__ == <span class="stringliteral">&quot;__main__&quot;</span>:</div>
<div class="line"> </div>
<div class="line">    <span class="comment"># Train a reinforcement learning agent on a gym environment.</span></div>
<div class="line">    run_training(agent=<span class="stringliteral">&quot;DDQN&quot;</span>, env=<span class="stringliteral">&quot;ALE/Pong-v5&quot;</span>, seed=0)</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md17"></a>
6. Running your First Experiment</h2>
<p>While, you could use the individual scripts such as <code>run_training.py</code> and <code>run_demo.py</code> manually, ReLab allows you to run entire experiments. An experiment automates training, evaluation, and result visualization across multiple agents, environments, and seeds. Here's a breakdown of what the script does:</p>
<ol type="1">
<li><b>Training Agents</b>: For each combination of agent, environment, and seed, the script launches training jobs either locally or using Slurm (a workload manager for distributed systems).</li>
<li><b>Policy Demonstrations</b>: After training, it generates GIFs to visually demonstrate the learned policies for each agent-environment-seed combination.</li>
<li><b>Performance Analysis</b>: The script creates performance graphs (e.g., mean episodic rewards with standard deviations) for each environment, summarizing how all agents performed.</li>
<li><b>Parallelization</b>: Jobs are managed efficiently either on the local machine (with multiple workers) or on a Slurm cluster, depending on the user’s choice.</li>
</ol>
<h4><a class="anchor" id="autotoc_md18"></a>
Example Usage:</h4>
<ul>
<li>Specify agents, environments, and seeds using command-line arguments. For example: <div class="fragment"><div class="line">python run_experiments.py --agents DQN RainbowDQN --envs ALE/Pong-v5 --seeds 0 1 2 --no-local</div>
</div><!-- fragment --></li>
<li>Use the <code>--no-local</code> flag to run experiments using Slurm. Omitting it defaults to run locally.</li>
</ul>
<p>This script ensures a streamlined workflow for conducting experiments, from training to visualization, with minimal manual intervention!</p>
<h2><a class="anchor" id="autotoc_md19"></a>
7. What's Next?</h2>
<p>For more details, you can explore the official documentation, which provides an in-depth explanation of all ReLab’s classes. Additionally, the Python scripts in the <code>scripts</code> directory offer practical examples to help you understand how ReLab works. These resources are great starting points for deepening your understanding and making the most out of ReLab! </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.6 </li>
  </ul>
</div>
</body>
</html>
